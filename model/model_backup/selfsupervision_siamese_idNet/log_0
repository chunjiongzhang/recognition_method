opt = Namespace(PCB=False, alpha=1.0, batchsize=48, color_jitter=False, data_dir='data/market/pytorch', erasing_p=0.5, gpu_ids='0', lr=0.1, name='sggnn', net_loss_model=0, train_all=True, use_dense=True)
net_loss_model = 0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=0), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f15a6529f98>]
7.152557373046875e-07
model_siamese structure
Epoch 0/99
----------
train Loss_id: 0.1622 Loss_verif: 0.0136  Acc_id: 0.3466 Verif_Acc: 0.6591 
Epoch 1/99
----------
train Loss_id: 0.0564 Loss_verif: 0.0119  Acc_id: 0.7252 Verif_Acc: 0.8192 
Epoch 2/99
----------
train Loss_id: 0.0369 Loss_verif: 0.0107  Acc_id: 0.8317 Verif_Acc: 0.8964 
Epoch 3/99
----------
train Loss_id: 0.0294 Loss_verif: 0.0098  Acc_id: 0.8737 Verif_Acc: 0.9177 
Epoch 4/99
----------
train Loss_id: 0.0249 Loss_verif: 0.0093  Acc_id: 0.8995 Verif_Acc: 0.9335 
Epoch 5/99
----------
train Loss_id: 0.0231 Loss_verif: 0.0088  Acc_id: 0.9067 Verif_Acc: 0.9475 
Epoch 6/99
----------
train Loss_id: 0.0205 Loss_verif: 0.0085  Acc_id: 0.9224 Verif_Acc: 0.9463 
Epoch 7/99
----------
train Loss_id: 0.0196 Loss_verif: 0.0084  Acc_id: 0.9273 Verif_Acc: 0.9479 
Epoch 8/99
----------
train Loss_id: 0.0182 Loss_verif: 0.0081  Acc_id: 0.9328 Verif_Acc: 0.9584 
Epoch 9/99
----------
train Loss_id: 0.0187 Loss_verif: 0.0080  Acc_id: 0.9292 Verif_Acc: 0.9552 
Epoch 10/99
----------
train Loss_id: 0.0176 Loss_verif: 0.0078  Acc_id: 0.9375 Verif_Acc: 0.9594 
Epoch 11/99
----------
train Loss_id: 0.0172 Loss_verif: 0.0078  Acc_id: 0.9394 Verif_Acc: 0.9605 
Epoch 12/99
----------
train Loss_id: 0.0164 Loss_verif: 0.0077  Acc_id: 0.9439 Verif_Acc: 0.9608 
Epoch 13/99
----------
train Loss_id: 0.0160 Loss_verif: 0.0076  Acc_id: 0.9472 Verif_Acc: 0.9624 
Epoch 14/99
----------
train Loss_id: 0.0154 Loss_verif: 0.0076  Acc_id: 0.9505 Verif_Acc: 0.9624 
Epoch 15/99
----------
train Loss_id: 0.0163 Loss_verif: 0.0076  Acc_id: 0.9457 Verif_Acc: 0.9614 
Epoch 16/99
----------
train Loss_id: 0.0154 Loss_verif: 0.0075  Acc_id: 0.9508 Verif_Acc: 0.9638 
Epoch 17/99
----------
train Loss_id: 0.0145 Loss_verif: 0.0075  Acc_id: 0.9568 Verif_Acc: 0.9632 
Epoch 18/99
----------
train Loss_id: 0.0144 Loss_verif: 0.0074  Acc_id: 0.9553 Verif_Acc: 0.9665 
Epoch 19/99
----------
train Loss_id: 0.0146 Loss_verif: 0.0074  Acc_id: 0.9537 Verif_Acc: 0.9672 
Epoch 20/99
----------
train Loss_id: 0.0148 Loss_verif: 0.0074  Acc_id: 0.9542 Verif_Acc: 0.9647 
Epoch 21/99
----------
train Loss_id: 0.0145 Loss_verif: 0.0073  Acc_id: 0.9549 Verif_Acc: 0.9695 
Epoch 22/99
----------
train Loss_id: 0.0147 Loss_verif: 0.0073  Acc_id: 0.9543 Verif_Acc: 0.9652 
Epoch 23/99
----------
train Loss_id: 0.0140 Loss_verif: 0.0073  Acc_id: 0.9585 Verif_Acc: 0.9675 
Epoch 24/99
----------
train Loss_id: 0.0132 Loss_verif: 0.0072  Acc_id: 0.9642 Verif_Acc: 0.9681 
Epoch 25/99
----------
train Loss_id: 0.0130 Loss_verif: 0.0072  Acc_id: 0.9644 Verif_Acc: 0.9673 
Epoch 26/99
----------
train Loss_id: 0.0132 Loss_verif: 0.0072  Acc_id: 0.9630 Verif_Acc: 0.9698 
Epoch 27/99
----------
train Loss_id: 0.0137 Loss_verif: 0.0071  Acc_id: 0.9590 Verif_Acc: 0.9722 
Epoch 28/99
----------
train Loss_id: 0.0137 Loss_verif: 0.0072  Acc_id: 0.9597 Verif_Acc: 0.9667 
Epoch 29/99
----------
train Loss_id: 0.0142 Loss_verif: 0.0072  Acc_id: 0.9563 Verif_Acc: 0.9690 
Epoch 30/99
----------
train Loss_id: 0.0099 Loss_verif: 0.0069  Acc_id: 0.9845 Verif_Acc: 0.9786 
Epoch 31/99
----------
train Loss_id: 0.0085 Loss_verif: 0.0068  Acc_id: 0.9914 Verif_Acc: 0.9820 
Epoch 32/99
----------
train Loss_id: 0.0081 Loss_verif: 0.0067  Acc_id: 0.9934 Verif_Acc: 0.9823 
Epoch 33/99
----------
train Loss_id: 0.0080 Loss_verif: 0.0067  Acc_id: 0.9944 Verif_Acc: 0.9828 
Epoch 34/99
----------
train Loss_id: 0.0080 Loss_verif: 0.0066  Acc_id: 0.9940 Verif_Acc: 0.9819 
Epoch 35/99
----------
train Loss_id: 0.0077 Loss_verif: 0.0065  Acc_id: 0.9952 Verif_Acc: 0.9844 
Epoch 36/99
----------
train Loss_id: 0.0077 Loss_verif: 0.0065  Acc_id: 0.9958 Verif_Acc: 0.9833 
Epoch 37/99
----------
train Loss_id: 0.0076 Loss_verif: 0.0064  Acc_id: 0.9957 Verif_Acc: 0.9824 
Epoch 38/99
----------
train Loss_id: 0.0076 Loss_verif: 0.0064  Acc_id: 0.9958 Verif_Acc: 0.9838 
Epoch 39/99
----------
train Loss_id: 0.0075 Loss_verif: 0.0063  Acc_id: 0.9957 Verif_Acc: 0.9846 
Epoch 40/99
----------
train Loss_id: 0.0075 Loss_verif: 0.0063  Acc_id: 0.9960 Verif_Acc: 0.9843 
Epoch 41/99
----------
train Loss_id: 0.0075 Loss_verif: 0.0063  Acc_id: 0.9959 Verif_Acc: 0.9832 
Epoch 42/99
----------
train Loss_id: 0.0074 Loss_verif: 0.0062  Acc_id: 0.9960 Verif_Acc: 0.9826 
Epoch 43/99
----------
train Loss_id: 0.0074 Loss_verif: 0.0062  Acc_id: 0.9962 Verif_Acc: 0.9817 
Epoch 44/99
----------
train Loss_id: 0.0074 Loss_verif: 0.0062  Acc_id: 0.9966 Verif_Acc: 0.9835 
Epoch 45/99
----------
train Loss_id: 0.0072 Loss_verif: 0.0061  Acc_id: 0.9971 Verif_Acc: 0.9827 
Epoch 46/99
----------
train Loss_id: 0.0073 Loss_verif: 0.0061  Acc_id: 0.9969 Verif_Acc: 0.9817 
Epoch 47/99
----------
train Loss_id: 0.0073 Loss_verif: 0.0061  Acc_id: 0.9965 Verif_Acc: 0.9801 
Epoch 48/99
----------
train Loss_id: 0.0072 Loss_verif: 0.0060  Acc_id: 0.9971 Verif_Acc: 0.9826 
Epoch 49/99
----------
train Loss_id: 0.0072 Loss_verif: 0.0060  Acc_id: 0.9975 Verif_Acc: 0.9825 
Epoch 50/99
----------
train Loss_id: 0.0071 Loss_verif: 0.0060  Acc_id: 0.9975 Verif_Acc: 0.9818 
Epoch 51/99
----------
train Loss_id: 0.0071 Loss_verif: 0.0059  Acc_id: 0.9970 Verif_Acc: 0.9817 
Epoch 52/99
----------
train Loss_id: 0.0071 Loss_verif: 0.0059  Acc_id: 0.9976 Verif_Acc: 0.9816 
Epoch 53/99
----------
train Loss_id: 0.0070 Loss_verif: 0.0059  Acc_id: 0.9977 Verif_Acc: 0.9830 
Epoch 54/99
----------
train Loss_id: 0.0070 Loss_verif: 0.0059  Acc_id: 0.9978 Verif_Acc: 0.9834 
Epoch 55/99
----------
train Loss_id: 0.0069 Loss_verif: 0.0058  Acc_id: 0.9978 Verif_Acc: 0.9817 
Epoch 56/99
----------
train Loss_id: 0.0070 Loss_verif: 0.0058  Acc_id: 0.9974 Verif_Acc: 0.9795 
Epoch 57/99
----------
train Loss_id: 0.0070 Loss_verif: 0.0058  Acc_id: 0.9975 Verif_Acc: 0.9807 
Epoch 58/99
----------
train Loss_id: 0.0069 Loss_verif: 0.0058  Acc_id: 0.9980 Verif_Acc: 0.9793 
Epoch 59/99
----------
train Loss_id: 0.0069 Loss_verif: 0.0057  Acc_id: 0.9978 Verif_Acc: 0.9811 
Epoch 60/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9977 Verif_Acc: 0.9809 
Epoch 61/99
----------
train Loss_id: 0.0069 Loss_verif: 0.0057  Acc_id: 0.9981 Verif_Acc: 0.9824 
Epoch 62/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9982 Verif_Acc: 0.9801 
Epoch 63/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9981 Verif_Acc: 0.9800 
Epoch 64/99
----------
train Loss_id: 0.0069 Loss_verif: 0.0057  Acc_id: 0.9978 Verif_Acc: 0.9793 
Epoch 65/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0057  Acc_id: 0.9982 Verif_Acc: 0.9804 
Epoch 66/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9982 Verif_Acc: 0.9793 
Epoch 67/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9980 Verif_Acc: 0.9819 
Epoch 68/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9983 Verif_Acc: 0.9795 
Epoch 69/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9981 Verif_Acc: 0.9802 
Epoch 70/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9979 Verif_Acc: 0.9802 
Epoch 71/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0057  Acc_id: 0.9985 Verif_Acc: 0.9830 
Epoch 72/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9982 Verif_Acc: 0.9814 
Epoch 73/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9977 Verif_Acc: 0.9797 
Epoch 74/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0057  Acc_id: 0.9982 Verif_Acc: 0.9816 
Epoch 75/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9984 Verif_Acc: 0.9813 
Epoch 76/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9982 Verif_Acc: 0.9799 
Epoch 77/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0056  Acc_id: 0.9982 Verif_Acc: 0.9828 
Epoch 78/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9982 Verif_Acc: 0.9796 
Epoch 79/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0057  Acc_id: 0.9984 Verif_Acc: 0.9794 
Epoch 80/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9982 Verif_Acc: 0.9786 
Epoch 81/99
----------
train Loss_id: 0.0068 Loss_verif: 0.0057  Acc_id: 0.9979 Verif_Acc: 0.9781 
Epoch 82/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9982 Verif_Acc: 0.9810 
Epoch 83/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9981 Verif_Acc: 0.9794 
Epoch 84/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0057  Acc_id: 0.9983 Verif_Acc: 0.9816 
Epoch 85/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9985 Verif_Acc: 0.9805 
Epoch 86/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9984 Verif_Acc: 0.9811 
Epoch 87/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9986 Verif_Acc: 0.9792 
Epoch 88/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0057  Acc_id: 0.9984 Verif_Acc: 0.9782 
Epoch 89/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9981 Verif_Acc: 0.9800 
Epoch 90/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9982 Verif_Acc: 0.9796 
Epoch 91/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9982 Verif_Acc: 0.9804 
Epoch 92/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9984 Verif_Acc: 0.9817 
Epoch 93/99
----------
train Loss_id: 0.0066 Loss_verif: 0.0056  Acc_id: 0.9985 Verif_Acc: 0.9823 
Epoch 94/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9980 Verif_Acc: 0.9791 
Epoch 95/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9983 Verif_Acc: 0.9802 
Epoch 96/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9980 Verif_Acc: 0.9792 
Epoch 97/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9982 Verif_Acc: 0.9817 
Epoch 98/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9986 Verif_Acc: 0.9805 
Epoch 99/99
----------
train Loss_id: 0.0067 Loss_verif: 0.0056  Acc_id: 0.9983 Verif_Acc: 0.9787 
best_epoch = 71     best_loss = 0.006185693058101564     best_acc = 0.990753745318352
Training complete in 196m 36s
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
opt = Namespace(PCB=False, batchsize=48, fp16=False, gpu_ids='0', name='sggnn', test_dir='data/market/pytorch', use_dense=True, which_epoch='best_siamese')
opt.gpu_ids = 0
opt.which_epoch = best_siamese
opt.test_dir = data/market/pytorch
opt.name = sggnn
opt.batchsize = 48
opt.use_dense = True
opt.PCB = False
opt.fp16 = False
-------test-----------
load easy pretrained model: ./model/sggnn/net_best_siamese.pth
torch.Size([3368, 512])
i =    0    CMC_tmp[0] = 1  real-time rank1 = 1.0000  avg rank1 = 1.0000
i =  100    CMC_tmp[0] = 0  real-time rank1 = 0.8119  avg rank1 = 0.8218
i =  200    CMC_tmp[0] = 1  real-time rank1 = 0.8317  avg rank1 = 0.8308
i =  300    CMC_tmp[0] = 1  real-time rank1 = 0.7921  avg rank1 = 0.8206
i =  400    CMC_tmp[0] = 1  real-time rank1 = 0.7624  avg rank1 = 0.8080
i =  500    CMC_tmp[0] = 1  real-time rank1 = 0.9406  avg rank1 = 0.8363
i =  600    CMC_tmp[0] = 1  real-time rank1 = 0.8812  avg rank1 = 0.8453
i =  700    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.8588
i =  800    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.8689
i =  900    CMC_tmp[0] = 1  real-time rank1 = 0.9505  avg rank1 = 0.8790
i = 1000    CMC_tmp[0] = 1  real-time rank1 = 0.8614  avg rank1 = 0.8781
i = 1100    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.8837
i = 1200    CMC_tmp[0] = 1  real-time rank1 = 0.9208  avg rank1 = 0.8876
i = 1300    CMC_tmp[0] = 1  real-time rank1 = 0.9505  avg rank1 = 0.8932
i = 1400    CMC_tmp[0] = 1  real-time rank1 = 0.9208  avg rank1 = 0.8958
i = 1500    CMC_tmp[0] = 1  real-time rank1 = 0.9505  avg rank1 = 0.9001
i = 1600    CMC_tmp[0] = 1  real-time rank1 = 0.8812  avg rank1 = 0.8994
i = 1700    CMC_tmp[0] = 1  real-time rank1 = 0.9604  avg rank1 = 0.9036
i = 1800    CMC_tmp[0] = 1  real-time rank1 = 0.9406  avg rank1 = 0.9062
i = 1900    CMC_tmp[0] = 1  real-time rank1 = 0.8713  avg rank1 = 0.9048
i = 2000    CMC_tmp[0] = 1  real-time rank1 = 0.9703  avg rank1 = 0.9085
i = 2100    CMC_tmp[0] = 1  real-time rank1 = 0.9703  avg rank1 = 0.9119
i = 2200    CMC_tmp[0] = 1  real-time rank1 = 0.9010  avg rank1 = 0.9119
i = 2300    CMC_tmp[0] = 1  real-time rank1 = 0.8911  avg rank1 = 0.9113
i = 2400    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.9125
i = 2500    CMC_tmp[0] = 1  real-time rank1 = 0.8812  avg rank1 = 0.9116
i = 2600    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.9127
i = 2700    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.9137
i = 2800    CMC_tmp[0] = 0  real-time rank1 = 0.9208  avg rank1 = 0.9143
i = 2900    CMC_tmp[0] = 1  real-time rank1 = 0.9703  avg rank1 = 0.9166
i = 3000    CMC_tmp[0] = 1  real-time rank1 = 0.9604  avg rank1 = 0.9184
i = 3100    CMC_tmp[0] = 1  real-time rank1 = 0.9703  avg rank1 = 0.9203
i = 3200    CMC_tmp[0] = 1  real-time rank1 = 0.9703  avg rank1 = 0.9222
i = 3300    CMC_tmp[0] = 1  real-time rank1 = 0.9604  avg rank1 = 0.9237
i = 3367    CMC_tmp[0] = 1  real-time rank1 = 0.9412  avg rank1 = 0.9243
Rank@1:0.924287 Rank@5:0.975059 Rank@10:0.984561 mAP:0.820305
calculate initial distance
Reranking complete in 1m 6s
top1:0.936164 top5:0.969121 top10:0.977138 mAP:0.914827
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
opt = Namespace(PCB=False, batchsize=48, fp16=False, gpu_ids='0', name='sggnn', test_dir='data/market/pytorch', use_dense=True, which_epoch='last_siamese')
opt.gpu_ids = 0
opt.which_epoch = last_siamese
opt.test_dir = data/market/pytorch
opt.name = sggnn
opt.batchsize = 48
opt.use_dense = True
opt.PCB = False
opt.fp16 = False
-------test-----------
load easy pretrained model: ./model/sggnn/net_last_siamese.pth
torch.Size([3368, 512])
i =    0    CMC_tmp[0] = 1  real-time rank1 = 1.0000  avg rank1 = 1.0000
i =  100    CMC_tmp[0] = 0  real-time rank1 = 0.8119  avg rank1 = 0.8218
i =  200    CMC_tmp[0] = 1  real-time rank1 = 0.8317  avg rank1 = 0.8308
i =  300    CMC_tmp[0] = 1  real-time rank1 = 0.7921  avg rank1 = 0.8206
i =  400    CMC_tmp[0] = 1  real-time rank1 = 0.7624  avg rank1 = 0.8080
i =  500    CMC_tmp[0] = 1  real-time rank1 = 0.9406  avg rank1 = 0.8363
i =  600    CMC_tmp[0] = 1  real-time rank1 = 0.8812  avg rank1 = 0.8453
i =  700    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.8588
i =  800    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.8689
i =  900    CMC_tmp[0] = 1  real-time rank1 = 0.9505  avg rank1 = 0.8790
i = 1000    CMC_tmp[0] = 1  real-time rank1 = 0.8614  avg rank1 = 0.8781
i = 1100    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.8837
i = 1200    CMC_tmp[0] = 1  real-time rank1 = 0.9208  avg rank1 = 0.8876
i = 1300    CMC_tmp[0] = 1  real-time rank1 = 0.9505  avg rank1 = 0.8932
i = 1400    CMC_tmp[0] = 1  real-time rank1 = 0.9208  avg rank1 = 0.8958
i = 1500    CMC_tmp[0] = 1  real-time rank1 = 0.9505  avg rank1 = 0.9001
i = 1600    CMC_tmp[0] = 1  real-time rank1 = 0.8812  avg rank1 = 0.8994
i = 1700    CMC_tmp[0] = 1  real-time rank1 = 0.9604  avg rank1 = 0.9036
i = 1800    CMC_tmp[0] = 1  real-time rank1 = 0.9406  avg rank1 = 0.9062
i = 1900    CMC_tmp[0] = 1  real-time rank1 = 0.8713  avg rank1 = 0.9048
i = 2000    CMC_tmp[0] = 1  real-time rank1 = 0.9703  avg rank1 = 0.9085
i = 2100    CMC_tmp[0] = 1  real-time rank1 = 0.9703  avg rank1 = 0.9119
i = 2200    CMC_tmp[0] = 1  real-time rank1 = 0.9010  avg rank1 = 0.9119
i = 2300    CMC_tmp[0] = 1  real-time rank1 = 0.8911  avg rank1 = 0.9113
i = 2400    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.9125
i = 2500    CMC_tmp[0] = 1  real-time rank1 = 0.8812  avg rank1 = 0.9116
i = 2600    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.9127
i = 2700    CMC_tmp[0] = 1  real-time rank1 = 0.9307  avg rank1 = 0.9137
i = 2800    CMC_tmp[0] = 0  real-time rank1 = 0.9208  avg rank1 = 0.9143
i = 2900    CMC_tmp[0] = 1  real-time rank1 = 0.9703  avg rank1 = 0.9166
i = 3000    CMC_tmp[0] = 1  real-time rank1 = 0.9604  avg rank1 = 0.9184
i = 3100    CMC_tmp[0] = 1  real-time rank1 = 0.9703  avg rank1 = 0.9203
i = 3200    CMC_tmp[0] = 1  real-time rank1 = 0.9703  avg rank1 = 0.9222
i = 3300    CMC_tmp[0] = 1  real-time rank1 = 0.9604  avg rank1 = 0.9237
i = 3367    CMC_tmp[0] = 1  real-time rank1 = 0.9412  avg rank1 = 0.9243
Rank@1:0.924287 Rank@5:0.975059 Rank@10:0.984561 mAP:0.820305
calculate initial distance
Reranking complete in 1m 5s
top1:0.936164 top5:0.969121 top10:0.977138 mAP:0.914827
opt = Namespace(PCB=False, alpha=1.0, batchsize=48, color_jitter=False, data_dir='data/market/pytorch', erasing_p=0.5, gpu_ids='0', lr=0.1, name='sggnn', net_loss_model=0, train_all=True, use_dense=True)
net_loss_model = 0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=0), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fe19df80f28>]
4.76837158203125e-07
model_siamese structure
Epoch 0/99
----------
